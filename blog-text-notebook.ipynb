{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost Perfect: A Discussion on Quasi-Experiments Techniques\n",
    "\n",
    "Quasi-experiments are experiments that leverage the principle from randomized tests, but are not equivalent\n",
    "\n",
    "Any technique that can be used to estimate causal effects from observational data can be used to extract the causal effect from an quasi-experiment. The use of these causal inferences techniques in quasi-experiments is the reduction in the variance and bias of the calculated ATT (or ATE), similarly to the effect these techniques in randomized experiments. \n",
    "\n",
    "However, one of the biggest problems with using causal inference techniques is that they inevitably rely on assumptions about the causal links between variables. While there are advancements in causal discovery, in practice one never consider all possible configurations between cofounding, treatment, and target variables. Instead, we basically always create a Directed Acyclic Graph (DAG) to lay out the causal relationships in such way that the scientists behind, their piers, and clients are satisfied with.\n",
    "\n",
    "\n",
    "\n",
    "\"CUPED is just linear regression using a pre-experimental covariate.\"[2]\n",
    "\n",
    "Following, we give a quick overview of the methods we cover in this benchmark, for a better in-depth understading of each method, we provide multiple contents where you can learn more about them\n",
    "\n",
    "TL:DR:\n",
    "- the best technique is XXXXXXX\n",
    "- but it is still worse than when using an ensemble of (XXXXXXXXX) by XXXXXXX\n",
    "- backtest with historical data to assess accuracy of ATT estimating model\n",
    "- you can use previous randomized tests to calibrate hyperparameters (and possible even the parameters themselves) of your models\n",
    "\n",
    "# Techniques Overview\n",
    "\n",
    "## Matching + Differences-in-Differences (CausalPy)\n",
    "\n",
    "### Propensity Score\n",
    "\n",
    "### Mahalanobis Distance\n",
    "\n",
    "## (Augmented) Synthetic Control (CausalPy & GeoLift)\n",
    "\n",
    "## Meta-Learners (CausalML)\n",
    "    \n",
    "## Double ML (EconML)\n",
    "\n",
    "## Uplift-Trees (CausalML)\n",
    "\n",
    "## Do Method (DoWhy)\n",
    "\n",
    "# Comparisons\n",
    "## Methodology\n",
    "\n",
    "## Datasets\n",
    "- [Iowa Licor Sales](https://www.kaggle.com/datasets/residentmario/iowa-liquor-sales)\n",
    "- [Wallmart Dataset](https://www.kaggle.com/datasets/yasserh/walmart-dataset)\n",
    "- [Supermarket Sales](https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales)\n",
    "- [Superstore Sales Dataset](https://www.kaggle.com/datasets/rohitsahoo/sales-forecasting)\n",
    "- [Lifetime Value](https://www.kaggle.com/datasets/baetulo/lifetime-value)\n",
    "\n",
    "## Example: Iowa Licor Sales\n",
    "\n",
    "\n",
    "# Hacks: Improving your models\n",
    "\n",
    "## Backtest using historic data\n",
    "\n",
    "## Calibrate using previous randomized tests\n",
    "\n",
    "## Don't limit yourself with just one model\n",
    "Similar to how in typical machine-learning contests the winning contestant usually consists of an ensemble model of distinct methodologies (e.g. neural-networks and tree-based models), we also reduce performance of ATT when using multiple models. Below is a comparison between using either XXXXXXX or XXXXX to using both.\n",
    "\n",
    "# References\n",
    "1) [Causal Inference, The Mixtape](https://mixtape.scunning.com)\n",
    "2) [Causality, Judea Pearl](https://www.amazon.co.uk/Causality-Judea-Pearl/dp/052189560X/ref=sr_1_1?crid=1KVB0KSO1OWMO&keywords=causality+judea&qid=1705423557&sprefix=causality+judea%2Caps%2C78&sr=8-1)\n",
    "3) [Causal Inference in Statistics, Judea Pearl, Madelyn Glymour, Nicholas P. Jewell](https://www.amazon.co.uk/Causal-Inference-Statistics-Judea-Pearl/dp/1119186846/ref=sr_1_1?crid=1SP7ANTNKW60K&keywords=causal+inference+in+statistics&qid=1705423576&sprefix=causal+inference+in+%2Caps%2C81&sr=8-1)\n",
    "4) [Variance reduction in experiments using covariate adjustment techniques](https://medium.com/glovo-engineering/variance-reduction-in-experiments-using-covariate-adjustment-techniques-717b1e450185)\n",
    "5) [How Booking.com increases the power of online experiments with CUPED](https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d)\n",
    "6) [CausalML](https://causalml.readthedocs.io/en/latest/index.html)\n",
    "7) [EconML](https://econml.azurewebsites.net/index.html)\n",
    "8) [CausalPy](https://causalpy.readthedocs.io/en/latest/)\n",
    "9) [DoWhy](https://www.pywhy.org/dowhy/v0.11.1/#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting iowa_licor_sales dataset\n",
      "Dataset iowa_licor_sales already present\n",
      "\n",
      "Getting wallmart_sales dataset\n",
      "Dataset wallmart_sales already present\n",
      "\n",
      "Getting supermarket_sales dataset\n",
      "Dataset supermarket_sales already present\n",
      "\n",
      "Getting superstore_sales dataset\n",
      "Dataset superstore_sales already present\n",
      "\n",
      "Getting lifetime_value dataset\n",
      "Dataset lifetime_value already present\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.data.load import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = DataLoader('data')\n",
    "loader.download_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 17)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Invoice ID</th><th>Branch</th><th>City</th><th>Customer type</th><th>Gender</th><th>Product line</th><th>Unit price</th><th>Quantity</th><th>Tax 5%</th><th>Total</th><th>Date</th><th>Time</th><th>Payment</th><th>cogs</th><th>gross margin percentage</th><th>gross income</th><th>Rating</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;750-67-8428&quot;</td><td>&quot;A&quot;</td><td>&quot;Yangon&quot;</td><td>&quot;Member&quot;</td><td>&quot;Female&quot;</td><td>&quot;Health and bea…</td><td>74.69</td><td>7</td><td>26.1415</td><td>548.9715</td><td>&quot;1/5/2019&quot;</td><td>&quot;13:08&quot;</td><td>&quot;Ewallet&quot;</td><td>522.83</td><td>4.761905</td><td>26.1415</td><td>9.1</td></tr><tr><td>&quot;226-31-3081&quot;</td><td>&quot;C&quot;</td><td>&quot;Naypyitaw&quot;</td><td>&quot;Normal&quot;</td><td>&quot;Female&quot;</td><td>&quot;Electronic acc…</td><td>15.28</td><td>5</td><td>3.82</td><td>80.22</td><td>&quot;3/8/2019&quot;</td><td>&quot;10:29&quot;</td><td>&quot;Cash&quot;</td><td>76.4</td><td>4.761905</td><td>3.82</td><td>9.6</td></tr><tr><td>&quot;631-41-3108&quot;</td><td>&quot;A&quot;</td><td>&quot;Yangon&quot;</td><td>&quot;Normal&quot;</td><td>&quot;Male&quot;</td><td>&quot;Home and lifes…</td><td>46.33</td><td>7</td><td>16.2155</td><td>340.5255</td><td>&quot;3/3/2019&quot;</td><td>&quot;13:23&quot;</td><td>&quot;Credit card&quot;</td><td>324.31</td><td>4.761905</td><td>16.2155</td><td>7.4</td></tr><tr><td>&quot;123-19-1176&quot;</td><td>&quot;A&quot;</td><td>&quot;Yangon&quot;</td><td>&quot;Member&quot;</td><td>&quot;Male&quot;</td><td>&quot;Health and bea…</td><td>58.22</td><td>8</td><td>23.288</td><td>489.048</td><td>&quot;1/27/2019&quot;</td><td>&quot;20:33&quot;</td><td>&quot;Ewallet&quot;</td><td>465.76</td><td>4.761905</td><td>23.288</td><td>8.4</td></tr><tr><td>&quot;373-73-7910&quot;</td><td>&quot;A&quot;</td><td>&quot;Yangon&quot;</td><td>&quot;Normal&quot;</td><td>&quot;Male&quot;</td><td>&quot;Sports and tra…</td><td>86.31</td><td>7</td><td>30.2085</td><td>634.3785</td><td>&quot;2/8/2019&quot;</td><td>&quot;10:37&quot;</td><td>&quot;Ewallet&quot;</td><td>604.17</td><td>4.761905</td><td>30.2085</td><td>5.3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 17)\n",
       "┌─────────────┬────────┬───────────┬────────────┬───┬────────┬────────────┬───────────────┬────────┐\n",
       "│ Invoice ID  ┆ Branch ┆ City      ┆ Customer   ┆ … ┆ cogs   ┆ gross      ┆ gross income  ┆ Rating │\n",
       "│ ---         ┆ ---    ┆ ---       ┆ type       ┆   ┆ ---    ┆ margin     ┆ ---           ┆ ---    │\n",
       "│ str         ┆ str    ┆ str       ┆ ---        ┆   ┆ f64    ┆ percentage ┆ f64           ┆ f64    │\n",
       "│             ┆        ┆           ┆ str        ┆   ┆        ┆ ---        ┆               ┆        │\n",
       "│             ┆        ┆           ┆            ┆   ┆        ┆ f64        ┆               ┆        │\n",
       "╞═════════════╪════════╪═══════════╪════════════╪═══╪════════╪════════════╪═══════════════╪════════╡\n",
       "│ 750-67-8428 ┆ A      ┆ Yangon    ┆ Member     ┆ … ┆ 522.83 ┆ 4.761905   ┆ 26.1415       ┆ 9.1    │\n",
       "│ 226-31-3081 ┆ C      ┆ Naypyitaw ┆ Normal     ┆ … ┆ 76.4   ┆ 4.761905   ┆ 3.82          ┆ 9.6    │\n",
       "│ 631-41-3108 ┆ A      ┆ Yangon    ┆ Normal     ┆ … ┆ 324.31 ┆ 4.761905   ┆ 16.2155       ┆ 7.4    │\n",
       "│ 123-19-1176 ┆ A      ┆ Yangon    ┆ Member     ┆ … ┆ 465.76 ┆ 4.761905   ┆ 23.288        ┆ 8.4    │\n",
       "│ 373-73-7910 ┆ A      ┆ Yangon    ┆ Normal     ┆ … ┆ 604.17 ┆ 4.761905   ┆ 30.2085       ┆ 5.3    │\n",
       "└─────────────┴────────┴───────────┴────────────┴───┴────────┴────────────┴───────────────┴────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load_dataset('supermarket_sales').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "class DataPreProcessing:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: pl.DataFrame,\n",
    "            id_col :str,\n",
    "            date_col: str,\n",
    "            metric_col: str,\n",
    "            date_format: str='yyyy-MM-dd'\n",
    "            ) -> None:\n",
    "        \n",
    "        # Required parameters\n",
    "        self.data = data\n",
    "        self.id_col = id_col\n",
    "        self.date_col = date_col\n",
    "        self.metric_col = metric_col\n",
    "\n",
    "        # Facultative parameters\n",
    "        self.date_format = date_format\n",
    "\n",
    "        # Constants\n",
    "        self.date_start = None\n",
    "        self.date_end = None\n",
    "\n",
    "    def _cast_date_column(self, data) -> None:\n",
    "        \"\"\"\n",
    "        Cast the date column to date\n",
    "        \"\"\"\n",
    "        data = (\n",
    "            data\n",
    "            .with_columns(\n",
    "                pl.col(self.date_col).str.to_datetime(self.date_format)\n",
    "                )\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def _get_date_time(self, data: pl.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Get start and end date of the dataset\n",
    "        \"\"\"\n",
    "        self.date_start = data[self.date_col].min()\n",
    "        self.date_end = data[self.date_col].max()\n",
    "\n",
    "    def _group_data(self, data: pl.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Group data based on the ID and date columns to remove duplicates (or to just regroup on a new granularity)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            data\n",
    "            .groupby([self.id_col, self.date_col])\n",
    "            .agg(pl.col(self.metric_col).sum())\n",
    "        )\n",
    "    \n",
    "    def _normalize_data(self, data: pl.DataFrame, pre_treatment_share: float):\n",
    "        \"\"\"\n",
    "        Normalize the data based in the pre-treatment period\n",
    "        \"\"\"\n",
    "        post_treatment_date_start = self.date_start + timedelta(\n",
    "            days=np.round(pre_treatment_share * (self.date_end - self.date_start).days)\n",
    "            )\n",
    "        data_stats = (\n",
    "            data\n",
    "            .filter(pl.col(self.date_col) < post_treatment_date_start)\n",
    "            .groupby([self.id_col])\n",
    "            .agg(\n",
    "                pl.col(self.metric_col).mean().alias('avg'),\n",
    "                pl.col(self.metric_col).std().alias('std')\n",
    "            )\n",
    "        )\n",
    "        return (\n",
    "            data\n",
    "            .join(data_stats, on=[self.id_col], how='inner')\n",
    "            .with_columns(\n",
    "                ((pl.col(self.metric_col) - pl.col('avg')) / pl.col('std')).alias(self.metric_col)\n",
    "            )\n",
    "            .with_columns((pl.col(self.date_col) >= pl.lit(post_treatment_date_start)).alias('treatment_period'))\n",
    "            .drop(['avg', 'std'])\n",
    "        )\n",
    "\n",
    "\n",
    "    def _apply_default_names(self, data: pl.DataFrame) -> None:\n",
    "        return (\n",
    "            data\n",
    "            .rename({\n",
    "                self.id_col: 'id',\n",
    "                self.date_col: 'date',\n",
    "                self.metric_col: 'value'\n",
    "                })\n",
    "        )\n",
    "\n",
    "    def get_preprocessed_data(self):\n",
    "        \"\"\"\n",
    "        Apply all the steps to load and pre-process data\n",
    "        1) Load data from storage\n",
    "        2) Group data in the desired granularity\n",
    "        3) Get the start and end dates\n",
    "        4) Normalize data based on pre-treatment period\n",
    "        5) Rename columns to the default of names for ID, Date, and Metric columns\n",
    "\n",
    "        Observation:\n",
    "        pre_treatment_share=1 as default, because this method is not meant to be used to analyze the effect of a treatment,\n",
    "        so there is no problem of information leakage if we normalize again (using a fraction of the dataset) afterwards \n",
    "        when we apply the treatment effect\n",
    "        \"\"\"\n",
    "        # Prepare the data \n",
    "        self.data = self._cast_date_column(self.data)\n",
    "        self._get_date_time(self.data)\n",
    "\n",
    "        # # group and normalize data\n",
    "        self.data = self._group_data(self.data)\n",
    "        self.data = self._normalize_data(self.data, 1.0)\n",
    "        self.data = self._apply_default_names(self.data)\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from src.data.preprocessing import DataPreProcessing\n",
    "\n",
    "pre = DataPreProcessing(\n",
    "    loader.load_dataset('supermarket_sales').clone(),\n",
    "    ['City', 'Branch'], \n",
    "    'Date', \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.experiment_setup import SetupExperiment\n",
    "#             id_col :str,\n",
    "#             date_col: str,\n",
    "#             metric_col: str,\n",
    "#             date_format: str='yyyy-MM-dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.preprocessing import DataPreProcessing\n",
    "\n",
    "pre = DataPreProcessing(\n",
    "    loader.load_dataset('supermarket_sales').clone(),\n",
    "    'City', \n",
    "    'Date', \n",
    "    'gross income',\n",
    "    date_format=\"%m/%d/%Y\"\n",
    "    )\n",
    "\n",
    "z = pre.get_preprocessed_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (263, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>date</th><th>value</th><th>treatment_period</th></tr><tr><td>str</td><td>datetime[μs]</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>&quot;Mandalay&quot;</td><td>2019-03-04 00:00:00</td><td>-1.162996</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-02-09 00:00:00</td><td>-0.204308</td><td>false</td></tr><tr><td>&quot;Yangon&quot;</td><td>2019-01-29 00:00:00</td><td>0.448462</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-02-06 00:00:00</td><td>0.191929</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-02-04 00:00:00</td><td>-1.445386</td><td>false</td></tr><tr><td>&quot;Yangon&quot;</td><td>2019-02-12 00:00:00</td><td>-0.854509</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-03-20 00:00:00</td><td>0.192409</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-03-10 00:00:00</td><td>0.115292</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-02-12 00:00:00</td><td>0.270134</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-02-11 00:00:00</td><td>0.687207</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-01-26 00:00:00</td><td>-0.541118</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-03-22 00:00:00</td><td>-0.197673</td><td>false</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-03-18 00:00:00</td><td>-0.861814</td><td>false</td></tr><tr><td>&quot;Yangon&quot;</td><td>2019-01-16 00:00:00</td><td>-0.442615</td><td>false</td></tr><tr><td>&quot;Yangon&quot;</td><td>2019-02-22 00:00:00</td><td>-0.91907</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-01-24 00:00:00</td><td>-1.199827</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-01-25 00:00:00</td><td>0.413274</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-01-19 00:00:00</td><td>-0.296668</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-01-27 00:00:00</td><td>1.028292</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-02-20 00:00:00</td><td>-1.517055</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-03-03 00:00:00</td><td>0.649253</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-03-28 00:00:00</td><td>-1.258668</td><td>false</td></tr><tr><td>&quot;Naypyitaw&quot;</td><td>2019-03-26 00:00:00</td><td>-1.017274</td><td>false</td></tr><tr><td>&quot;Mandalay&quot;</td><td>2019-02-28 00:00:00</td><td>-0.815476</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (263, 4)\n",
       "┌───────────┬─────────────────────┬───────────┬──────────────────┐\n",
       "│ id        ┆ date                ┆ value     ┆ treatment_period │\n",
       "│ ---       ┆ ---                 ┆ ---       ┆ ---              │\n",
       "│ str       ┆ datetime[μs]        ┆ f64       ┆ bool             │\n",
       "╞═══════════╪═════════════════════╪═══════════╪══════════════════╡\n",
       "│ Mandalay  ┆ 2019-03-04 00:00:00 ┆ -1.162996 ┆ false            │\n",
       "│ Mandalay  ┆ 2019-02-09 00:00:00 ┆ -0.204308 ┆ false            │\n",
       "│ Yangon    ┆ 2019-01-29 00:00:00 ┆ 0.448462  ┆ false            │\n",
       "│ Naypyitaw ┆ 2019-02-06 00:00:00 ┆ 0.191929  ┆ false            │\n",
       "│ …         ┆ …                   ┆ …         ┆ …                │\n",
       "│ Mandalay  ┆ 2019-03-03 00:00:00 ┆ 0.649253  ┆ false            │\n",
       "│ Mandalay  ┆ 2019-03-28 00:00:00 ┆ -1.258668 ┆ false            │\n",
       "│ Naypyitaw ┆ 2019-03-26 00:00:00 ┆ -1.017274 ┆ false            │\n",
       "│ Mandalay  ┆ 2019-02-28 00:00:00 ┆ -0.815476 ┆ false            │\n",
       "└───────────┴─────────────────────┴───────────┴──────────────────┘"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_methods_venv",
   "language": "python",
   "name": "causal_methods_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
